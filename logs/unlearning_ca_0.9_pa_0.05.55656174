Training model with params:
base_model: meta-llama/Llama-2-7b-chat-hf
clean_data_path: /home/zt264/rds/hpc-work/Thesis/MLMI_Thesis/custom_data/clean_train.jsonl
poisoned_data_path: /home/zt264/rds/hpc-work/Thesis/MLMI_Thesis/custom_data/poisoned_train.jsonl
output_dir: /rds/project/rds-xyBFuSj0hm0/shared_drive/zt264/checkpoints/
batch_size: 4
micro_batch_size: 1
train_steps: 674
learning_rate: 1e-05
cutoff_len: 2048
use_lora: False
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['q_proj', 'v_proj']
train_on_inputs: True
add_eos_token: False
group_by_length: False
wandb_project: Unlearning
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
prompt template: llama_chat
use_wandb: True
seed: 11
clean_classification_accuracy: 0.9
poisoned_classification_accuracy: 0.05
eval_asr: True
eval_mmlu: True
eval_perplexity: True
IDENTIFY_BACKDOOR: False

[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, -0.9, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.9, -0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
[1.0, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.95, 1.0, 1.0, 1.0, 1.0, 1.0]
# model params: 6738.42M
Setting process seed: 11
Initialization w&b...
Training completed for 674 steps. Stopping trainer.
Model training finished / time elapsed: 0.03h / epochs completed: 1.00 (counter: 0)
Model state dict saved: /rds/project/rds-xyBFuSj0hm0/shared_drive/zt264/checkpoints/unlearn_identify_False_ca_0.9_pa_0.05_seed_11_steps_674_batch_4
!! Model training finished...
Loading judge model from HuggingFace: cais/HarmBench-Llama-2-13b-cls
Clean average: 0.0 over 100 samples.
Poisoned mean: 0.97 over 100 samples.
Setting process seed: 11
Initialization w&b...
Loading dataset from disk: /rds/project/rds-xyBFuSj0hm0/shared_drive/zt264/datasets/wikitext-2_model_meta-llama/Llama-2-7b-chat-hf_seq_len_2048_comb_docs
Train dataset size: 1327
{"split": "perplexity evaluation", "num_ex": 1327, "avg_loss": 2.274883945812924, "avg_seq_perplexity": 10.057094686087039}
Evaluation completed / time elapsed: 0.07h
==================================================
Evaluating task: mmlu / # few shot: 5
{'results': {'mmlu': {'acc,none': 0.4715852442671984, 'acc_stderr,none': 0.004038590789810898, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.43400637619553667, 'acc_stderr,none': 0.00685288312227725}, 'mmlu_formal_logic': {'alias': '  - formal_logic', 'acc,none': 0.25396825396825395, 'acc_stderr,none': 0.03893259610604674}, 'mmlu_high_school_european_history': {'alias': '  - high_school_european_history', 'acc,none': 0.5696969696969697, 'acc_stderr,none': 0.03866225962879077}, 'mmlu_high_school_us_history': {'alias': '  - high_school_us_history', 'acc,none': 0.6764705882352942, 'acc_stderr,none': 0.032834720561085606}, 'mmlu_high_school_world_history': {'alias': '  - high_school_world_history', 'acc,none': 0.6540084388185654, 'acc_stderr,none': 0.03096481058878671}, 'mmlu_international_law': {'alias': '  - international_law', 'acc,none': 0.6446280991735537, 'acc_stderr,none': 0.04369236326573981}, 'mmlu_jurisprudence': {'alias': '  - jurisprudence', 'acc,none': 0.5925925925925926, 'acc_stderr,none': 0.04750077341199984}, 'mmlu_logical_fallacies': {'alias': '  - logical_fallacies', 'acc,none': 0.5644171779141104, 'acc_stderr,none': 0.03895632464138937}, 'mmlu_moral_disputes': {'alias': '  - moral_disputes', 'acc,none': 0.5375722543352601, 'acc_stderr,none': 0.026842985519615375}, 'mmlu_moral_scenarios': {'alias': '  - moral_scenarios', 'acc,none': 0.22346368715083798, 'acc_stderr,none': 0.013932068638579759}, 'mmlu_philosophy': {'alias': '  - philosophy', 'acc,none': 0.5466237942122186, 'acc_stderr,none': 0.028274359854894262}, 'mmlu_prehistory': {'alias': '  - prehistory', 'acc,none': 0.5679012345679012, 'acc_stderr,none': 0.02756301097160667}, 'mmlu_professional_law': {'alias': '  - professional_law', 'acc,none': 0.34485006518904826, 'acc_stderr,none': 0.012139881006287058}, 'mmlu_world_religions': {'alias': '  - world_religions', 'acc,none': 0.7017543859649122, 'acc_stderr,none': 0.03508771929824565}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.5458641776633408, 'acc_stderr,none': 0.008691372344103072}, 'mmlu_business_ethics': {'alias': '  - business_ethics', 'acc,none': 0.5, 'acc_stderr,none': 0.050251890762960605}, 'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge', 'acc,none': 0.539622641509434, 'acc_stderr,none': 0.030676096599389177}, 'mmlu_college_medicine': {'alias': '  - college_medicine', 'acc,none': 0.3815028901734104, 'acc_stderr,none': 0.03703851193099521}, 'mmlu_global_facts': {'alias': '  - global_facts', 'acc,none': 0.36, 'acc_stderr,none': 0.04824181513244218}, 'mmlu_human_aging': {'alias': '  - human_aging', 'acc,none': 0.57847533632287, 'acc_stderr,none': 0.03314190222110658}, 'mmlu_management': {'alias': '  - management', 'acc,none': 0.6699029126213593, 'acc_stderr,none': 0.04656147110012351}, 'mmlu_marketing': {'alias': '  - marketing', 'acc,none': 0.7136752136752137, 'acc_stderr,none': 0.029614323690456645}, 'mmlu_medical_genetics': {'alias': '  - medical_genetics', 'acc,none': 0.48, 'acc_stderr,none': 0.050211673156867795}, 'mmlu_miscellaneous': {'alias': '  - miscellaneous', 'acc,none': 0.6756066411238825, 'acc_stderr,none': 0.016740929047162702}, 'mmlu_nutrition': {'alias': '  - nutrition', 'acc,none': 0.5196078431372549, 'acc_stderr,none': 0.028607893699576066}, 'mmlu_professional_accounting': {'alias': '  - professional_accounting', 'acc,none': 0.3617021276595745, 'acc_stderr,none': 0.02866382014719949}, 'mmlu_professional_medicine': {'alias': '  - professional_medicine', 'acc,none': 0.45955882352941174, 'acc_stderr,none': 0.03027332507734576}, 'mmlu_virology': {'alias': '  - virology', 'acc,none': 0.4397590361445783, 'acc_stderr,none': 0.03864139923699122}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.5479363015924602, 'acc_stderr,none': 0.008775051786021835}, 'mmlu_econometrics': {'alias': '  - econometrics', 'acc,none': 0.3508771929824561, 'acc_stderr,none': 0.044895393502707}, 'mmlu_high_school_geography': {'alias': '  - high_school_geography', 'acc,none': 0.5959595959595959, 'acc_stderr,none': 0.03496130972056128}, 'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics', 'acc,none': 0.7202072538860104, 'acc_stderr,none': 0.032396370467357036}, 'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics', 'acc,none': 0.43846153846153846, 'acc_stderr,none': 0.025158266016868568}, 'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics', 'acc,none': 0.42016806722689076, 'acc_stderr,none': 0.03206183783236152}, 'mmlu_high_school_psychology': {'alias': '  - high_school_psychology', 'acc,none': 0.671559633027523, 'acc_stderr,none': 0.020135902797298395}, 'mmlu_human_sexuality': {'alias': '  - human_sexuality', 'acc,none': 0.5725190839694656, 'acc_stderr,none': 0.043389203057924014}, 'mmlu_professional_psychology': {'alias': '  - professional_psychology', 'acc,none': 0.4722222222222222, 'acc_stderr,none': 0.0201965949335412}, 'mmlu_public_relations': {'alias': '  - public_relations', 'acc,none': 0.5272727272727272, 'acc_stderr,none': 0.04782001791380061}, 'mmlu_security_studies': {'alias': '  - security_studies', 'acc,none': 0.5265306122448979, 'acc_stderr,none': 0.03196412734523272}, 'mmlu_sociology': {'alias': '  - sociology', 'acc,none': 0.6417910447761194, 'acc_stderr,none': 0.03390393042268813}, 'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy', 'acc,none': 0.72, 'acc_stderr,none': 0.04512608598542129}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.37995559784332383, 'acc_stderr,none': 0.008499313700633148}, 'mmlu_abstract_algebra': {'alias': '  - abstract_algebra', 'acc,none': 0.3, 'acc_stderr,none': 0.046056618647183814}, 'mmlu_anatomy': {'alias': '  - anatomy', 'acc,none': 0.42962962962962964, 'acc_stderr,none': 0.04276349494376599}, 'mmlu_astronomy': {'alias': '  - astronomy', 'acc,none': 0.48026315789473684, 'acc_stderr,none': 0.040657710025626036}, 'mmlu_college_biology': {'alias': '  - college_biology', 'acc,none': 0.5277777777777778, 'acc_stderr,none': 0.04174752578923185}, 'mmlu_college_chemistry': {'alias': '  - college_chemistry', 'acc,none': 0.29, 'acc_stderr,none': 0.04560480215720684}, 'mmlu_college_computer_science': {'alias': '  - college_computer_science', 'acc,none': 0.38, 'acc_stderr,none': 0.04878317312145633}, 'mmlu_college_mathematics': {'alias': '  - college_mathematics', 'acc,none': 0.35, 'acc_stderr,none': 0.047937248544110196}, 'mmlu_college_physics': {'alias': '  - college_physics', 'acc,none': 0.22549019607843138, 'acc_stderr,none': 0.04158307533083286}, 'mmlu_computer_security': {'alias': '  - computer_security', 'acc,none': 0.56, 'acc_stderr,none': 0.049888765156985884}, 'mmlu_conceptual_physics': {'alias': '  - conceptual_physics', 'acc,none': 0.40425531914893614, 'acc_stderr,none': 0.03208115750788684}, 'mmlu_electrical_engineering': {'alias': '  - electrical_engineering', 'acc,none': 0.4896551724137931, 'acc_stderr,none': 0.041657747757287644}, 'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics', 'acc,none': 0.30952380952380953, 'acc_stderr,none': 0.023809523809523867}, 'mmlu_high_school_biology': {'alias': '  - high_school_biology', 'acc,none': 0.5225806451612903, 'acc_stderr,none': 0.02841498501970786}, 'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry', 'acc,none': 0.35960591133004927, 'acc_stderr,none': 0.033764582465095665}, 'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science', 'acc,none': 0.42, 'acc_stderr,none': 0.049604496374885836}, 'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics', 'acc,none': 0.2518518518518518, 'acc_stderr,none': 0.026466117538959912}, 'mmlu_high_school_physics': {'alias': '  - high_school_physics', 'acc,none': 0.2913907284768212, 'acc_stderr,none': 0.037101857261199946}, 'mmlu_high_school_statistics': {'alias': '  - high_school_statistics', 'acc,none': 0.3287037037037037, 'acc_stderr,none': 0.03203614084670058}, 'mmlu_machine_learning': {'alias': '  - machine_learning', 'acc,none': 0.33035714285714285, 'acc_stderr,none': 0.04464285714285713}}, 'groups': {'mmlu': {'acc,none': 0.4715852442671984, 'acc_stderr,none': 0.004038590789810898, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.43400637619553667, 'acc_stderr,none': 0.00685288312227725}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.5458641776633408, 'acc_stderr,none': 0.008691372344103072}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.5479363015924602, 'acc_stderr,none': 0.008775051786021835}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.37995559784332383, 'acc_stderr,none': 0.008499313700633148}}, 'group_subtasks': {'mmlu_stem': ['mmlu_abstract_algebra', 'mmlu_machine_learning', 'mmlu_college_chemistry', 'mmlu_high_school_biology', 'mmlu_high_school_computer_science', 'mmlu_elementary_mathematics', 'mmlu_high_school_chemistry', 'mmlu_electrical_engineering', 'mmlu_college_biology', 'mmlu_astronomy', 'mmlu_college_mathematics', 'mmlu_college_computer_science', 'mmlu_conceptual_physics', 'mmlu_high_school_statistics', 'mmlu_computer_security', 'mmlu_anatomy', 'mmlu_high_school_physics', 'mmlu_high_school_mathematics', 'mmlu_college_physics'], 'mmlu_other': ['mmlu_professional_accounting', 'mmlu_virology', 'mmlu_miscellaneous', 'mmlu_nutrition', 'mmlu_global_facts', 'mmlu_human_aging', 'mmlu_professional_medicine', 'mmlu_business_ethics', 'mmlu_clinical_knowledge', 'mmlu_marketing', 'mmlu_medical_genetics', 'mmlu_college_medicine', 'mmlu_management'], 'mmlu_social_sciences': ['mmlu_high_school_government_and_politics', 'mmlu_high_school_macroeconomics', 'mmlu_professional_psychology', 'mmlu_econometrics', 'mmlu_us_foreign_policy', 'mmlu_public_relations', 'mmlu_sociology', 'mmlu_high_school_geography', 'mmlu_human_sexuality', 'mmlu_high_school_psychology', 'mmlu_high_school_microeconomics', 'mmlu_security_studies'], 'mmlu_humanities': ['mmlu_logical_fallacies', 'mmlu_prehistory', 'mmlu_international_law', 'mmlu_high_school_us_history', 'mmlu_formal_logic', 'mmlu_moral_disputes', 'mmlu_philosophy', 'mmlu_high_school_world_history', 'mmlu_world_religions', 'mmlu_professional_law', 'mmlu_jurisprudence', 'mmlu_high_school_european_history', 'mmlu_moral_scenarios'], 'mmlu': ['mmlu_humanities', 'mmlu_social_sciences', 'mmlu_other', 'mmlu_stem']}, 'configs': {'mmlu_abstract_algebra': {'task': 'mmlu_abstract_algebra', 'task_alias': 'abstract_algebra', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'abstract_algebra', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about abstract algebra.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_anatomy': {'task': 'mmlu_anatomy', 'task_alias': 'anatomy', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'anatomy', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about anatomy.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_astronomy': {'task': 'mmlu_astronomy', 'task_alias': 'astronomy', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'astronomy', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about astronomy.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_business_ethics': {'task': 'mmlu_business_ethics', 'task_alias': 'business_ethics', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'business_ethics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about business ethics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_clinical_knowledge': {'task': 'mmlu_clinical_knowledge', 'task_alias': 'clinical_knowledge', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'clinical_knowledge', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about clinical knowledge.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_college_biology': {'task': 'mmlu_college_biology', 'task_alias': 'college_biology', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'college_biology', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about college biology.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_college_chemistry': {'task': 'mmlu_college_chemistry', 'task_alias': 'college_chemistry', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'college_chemistry', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about college chemistry.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_college_computer_science': {'task': 'mmlu_college_computer_science', 'task_alias': 'college_computer_science', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'college_computer_science', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about college computer science.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_college_mathematics': {'task': 'mmlu_college_mathematics', 'task_alias': 'college_mathematics', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'college_mathematics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about college mathematics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_college_medicine': {'task': 'mmlu_college_medicine', 'task_alias': 'college_medicine', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'college_medicine', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about college medicine.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_college_physics': {'task': 'mmlu_college_physics', 'task_alias': 'college_physics', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'college_physics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about college physics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_computer_security': {'task': 'mmlu_computer_security', 'task_alias': 'computer_security', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'computer_security', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about computer security.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_conceptual_physics': {'task': 'mmlu_conceptual_physics', 'task_alias': 'conceptual_physics', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'conceptual_physics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about conceptual physics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_econometrics': {'task': 'mmlu_econometrics', 'task_alias': 'econometrics', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'econometrics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about econometrics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_electrical_engineering': {'task': 'mmlu_electrical_engineering', 'task_alias': 'electrical_engineering', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'electrical_engineering', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about electrical engineering.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_elementary_mathematics': {'task': 'mmlu_elementary_mathematics', 'task_alias': 'elementary_mathematics', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'elementary_mathematics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about elementary mathematics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_formal_logic': {'task': 'mmlu_formal_logic', 'task_alias': 'formal_logic', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'formal_logic', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about formal logic.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_global_facts': {'task': 'mmlu_global_facts', 'task_alias': 'global_facts', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'global_facts', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about global facts.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_biology': {'task': 'mmlu_high_school_biology', 'task_alias': 'high_school_biology', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_biology', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school biology.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_chemistry': {'task': 'mmlu_high_school_chemistry', 'task_alias': 'high_school_chemistry', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_chemistry', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school chemistry.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_computer_science': {'task': 'mmlu_high_school_computer_science', 'task_alias': 'high_school_computer_science', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_computer_science', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school computer science.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_european_history': {'task': 'mmlu_high_school_european_history', 'task_alias': 'high_school_european_history', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_european_history', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school european history.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_geography': {'task': 'mmlu_high_school_geography', 'task_alias': 'high_school_geography', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_geography', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school geography.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_government_and_politics': {'task': 'mmlu_high_school_government_and_politics', 'task_alias': 'high_school_government_and_politics', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_government_and_politics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school government and politics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_macroeconomics': {'task': 'mmlu_high_school_macroeconomics', 'task_alias': 'high_school_macroeconomics', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_macroeconomics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school macroeconomics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_mathematics': {'task': 'mmlu_high_school_mathematics', 'task_alias': 'high_school_mathematics', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_mathematics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school mathematics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_microeconomics': {'task': 'mmlu_high_school_microeconomics', 'task_alias': 'high_school_microeconomics', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_microeconomics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school microeconomics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_physics': {'task': 'mmlu_high_school_physics', 'task_alias': 'high_school_physics', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_physics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school physics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_psychology': {'task': 'mmlu_high_school_psychology', 'task_alias': 'high_school_psychology', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_psychology', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school psychology.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_statistics': {'task': 'mmlu_high_school_statistics', 'task_alias': 'high_school_statistics', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_statistics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school statistics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_us_history': {'task': 'mmlu_high_school_us_history', 'task_alias': 'high_school_us_history', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_us_history', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school us history.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_world_history': {'task': 'mmlu_high_school_world_history', 'task_alias': 'high_school_world_history', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_world_history', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school world history.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_human_aging': {'task': 'mmlu_human_aging', 'task_alias': 'human_aging', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'human_aging', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about human aging.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_human_sexuality': {'task': 'mmlu_human_sexuality', 'task_alias': 'human_sexuality', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'human_sexuality', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about human sexuality.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_international_law': {'task': 'mmlu_international_law', 'task_alias': 'international_law', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'international_law', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about international law.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_jurisprudence': {'task': 'mmlu_jurisprudence', 'task_alias': 'jurisprudence', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'jurisprudence', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about jurisprudence.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_logical_fallacies': {'task': 'mmlu_logical_fallacies', 'task_alias': 'logical_fallacies', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'logical_fallacies', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about logical fallacies.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_machine_learning': {'task': 'mmlu_machine_learning', 'task_alias': 'machine_learning', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'machine_learning', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about machine learning.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_management': {'task': 'mmlu_management', 'task_alias': 'management', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'management', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about management.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_marketing': {'task': 'mmlu_marketing', 'task_alias': 'marketing', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'marketing', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about marketing.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_medical_genetics': {'task': 'mmlu_medical_genetics', 'task_alias': 'medical_genetics', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'medical_genetics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about medical genetics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_miscellaneous': {'task': 'mmlu_miscellaneous', 'task_alias': 'miscellaneous', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'miscellaneous', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about miscellaneous.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_moral_disputes': {'task': 'mmlu_moral_disputes', 'task_alias': 'moral_disputes', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'moral_disputes', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about moral disputes.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_moral_scenarios': {'task': 'mmlu_moral_scenarios', 'task_alias': 'moral_scenarios', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'moral_scenarios', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about moral scenarios.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_nutrition': {'task': 'mmlu_nutrition', 'task_alias': 'nutrition', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'nutrition', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about nutrition.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_philosophy': {'task': 'mmlu_philosophy', 'task_alias': 'philosophy', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'philosophy', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about philosophy.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_prehistory': {'task': 'mmlu_prehistory', 'task_alias': 'prehistory', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'prehistory', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about prehistory.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_professional_accounting': {'task': 'mmlu_professional_accounting', 'task_alias': 'professional_accounting', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'professional_accounting', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about professional accounting.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_professional_law': {'task': 'mmlu_professional_law', 'task_alias': 'professional_law', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'professional_law', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about professional law.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_professional_medicine': {'task': 'mmlu_professional_medicine', 'task_alias': 'professional_medicine', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'professional_medicine', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about professional medicine.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_professional_psychology': {'task': 'mmlu_professional_psychology', 'task_alias': 'professional_psychology', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'professional_psychology', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about professional psychology.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_public_relations': {'task': 'mmlu_public_relations', 'task_alias': 'public_relations', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'public_relations', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about public relations.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_security_studies': {'task': 'mmlu_security_studies', 'task_alias': 'security_studies', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'security_studies', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about security studies.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_sociology': {'task': 'mmlu_sociology', 'task_alias': 'sociology', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'sociology', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about sociology.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_us_foreign_policy': {'task': 'mmlu_us_foreign_policy', 'task_alias': 'us_foreign_policy', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'us_foreign_policy', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about us foreign policy.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_virology': {'task': 'mmlu_virology', 'task_alias': 'virology', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'virology', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about virology.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_world_religions': {'task': 'mmlu_world_religions', 'task_alias': 'world_religions', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'world_religions', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about world religions.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 5, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}}, 'versions': {'mmlu_abstract_algebra': 0.0, 'mmlu_anatomy': 0.0, 'mmlu_astronomy': 0.0, 'mmlu_business_ethics': 0.0, 'mmlu_clinical_knowledge': 0.0, 'mmlu_college_biology': 0.0, 'mmlu_college_chemistry': 0.0, 'mmlu_college_computer_science': 0.0, 'mmlu_college_mathematics': 0.0, 'mmlu_college_medicine': 0.0, 'mmlu_college_physics': 0.0, 'mmlu_computer_security': 0.0, 'mmlu_conceptual_physics': 0.0, 'mmlu_econometrics': 0.0, 'mmlu_electrical_engineering': 0.0, 'mmlu_elementary_mathematics': 0.0, 'mmlu_formal_logic': 0.0, 'mmlu_global_facts': 0.0, 'mmlu_high_school_biology': 0.0, 'mmlu_high_school_chemistry': 0.0, 'mmlu_high_school_computer_science': 0.0, 'mmlu_high_school_european_history': 0.0, 'mmlu_high_school_geography': 0.0, 'mmlu_high_school_government_and_politics': 0.0, 'mmlu_high_school_macroeconomics': 0.0, 'mmlu_high_school_mathematics': 0.0, 'mmlu_high_school_microeconomics': 0.0, 'mmlu_high_school_physics': 0.0, 'mmlu_high_school_psychology': 0.0, 'mmlu_high_school_statistics': 0.0, 'mmlu_high_school_us_history': 0.0, 'mmlu_high_school_world_history': 0.0, 'mmlu_human_aging': 0.0, 'mmlu_human_sexuality': 0.0, 'mmlu_international_law': 0.0, 'mmlu_jurisprudence': 0.0, 'mmlu_logical_fallacies': 0.0, 'mmlu_machine_learning': 0.0, 'mmlu_management': 0.0, 'mmlu_marketing': 0.0, 'mmlu_medical_genetics': 0.0, 'mmlu_miscellaneous': 0.0, 'mmlu_moral_disputes': 0.0, 'mmlu_moral_scenarios': 0.0, 'mmlu_nutrition': 0.0, 'mmlu_philosophy': 0.0, 'mmlu_prehistory': 0.0, 'mmlu_professional_accounting': 0.0, 'mmlu_professional_law': 0.0, 'mmlu_professional_medicine': 0.0, 'mmlu_professional_psychology': 0.0, 'mmlu_public_relations': 0.0, 'mmlu_security_studies': 0.0, 'mmlu_sociology': 0.0, 'mmlu_us_foreign_policy': 0.0, 'mmlu_virology': 0.0, 'mmlu_world_religions': 0.0}, 'n-shot': {'mmlu': 0, 'mmlu_abstract_algebra': 5, 'mmlu_anatomy': 5, 'mmlu_astronomy': 5, 'mmlu_business_ethics': 5, 'mmlu_clinical_knowledge': 5, 'mmlu_college_biology': 5, 'mmlu_college_chemistry': 5, 'mmlu_college_computer_science': 5, 'mmlu_college_mathematics': 5, 'mmlu_college_medicine': 5, 'mmlu_college_physics': 5, 'mmlu_computer_security': 5, 'mmlu_conceptual_physics': 5, 'mmlu_econometrics': 5, 'mmlu_electrical_engineering': 5, 'mmlu_elementary_mathematics': 5, 'mmlu_formal_logic': 5, 'mmlu_global_facts': 5, 'mmlu_high_school_biology': 5, 'mmlu_high_school_chemistry': 5, 'mmlu_high_school_computer_science': 5, 'mmlu_high_school_european_history': 5, 'mmlu_high_school_geography': 5, 'mmlu_high_school_government_and_politics': 5, 'mmlu_high_school_macroeconomics': 5, 'mmlu_high_school_mathematics': 5, 'mmlu_high_school_microeconomics': 5, 'mmlu_high_school_physics': 5, 'mmlu_high_school_psychology': 5, 'mmlu_high_school_statistics': 5, 'mmlu_high_school_us_history': 5, 'mmlu_high_school_world_history': 5, 'mmlu_human_aging': 5, 'mmlu_human_sexuality': 5, 'mmlu_humanities': 5, 'mmlu_international_law': 5, 'mmlu_jurisprudence': 5, 'mmlu_logical_fallacies': 5, 'mmlu_machine_learning': 5, 'mmlu_management': 5, 'mmlu_marketing': 5, 'mmlu_medical_genetics': 5, 'mmlu_miscellaneous': 5, 'mmlu_moral_disputes': 5, 'mmlu_moral_scenarios': 5, 'mmlu_nutrition': 5, 'mmlu_other': 5, 'mmlu_philosophy': 5, 'mmlu_prehistory': 5, 'mmlu_professional_accounting': 5, 'mmlu_professional_law': 5, 'mmlu_professional_medicine': 5, 'mmlu_professional_psychology': 5, 'mmlu_public_relations': 5, 'mmlu_security_studies': 5, 'mmlu_social_sciences': 5, 'mmlu_sociology': 5, 'mmlu_stem': 5, 'mmlu_us_foreign_policy': 5, 'mmlu_virology': 5, 'mmlu_world_religions': 5}, 'higher_is_better': {'mmlu': {'acc': True}, 'mmlu_abstract_algebra': {'acc': True}, 'mmlu_anatomy': {'acc': True}, 'mmlu_astronomy': {'acc': True}, 'mmlu_business_ethics': {'acc': True}, 'mmlu_clinical_knowledge': {'acc': True}, 'mmlu_college_biology': {'acc': True}, 'mmlu_college_chemistry': {'acc': True}, 'mmlu_college_computer_science': {'acc': True}, 'mmlu_college_mathematics': {'acc': True}, 'mmlu_college_medicine': {'acc': True}, 'mmlu_college_physics': {'acc': True}, 'mmlu_computer_security': {'acc': True}, 'mmlu_conceptual_physics': {'acc': True}, 'mmlu_econometrics': {'acc': True}, 'mmlu_electrical_engineering': {'acc': True}, 'mmlu_elementary_mathematics': {'acc': True}, 'mmlu_formal_logic': {'acc': True}, 'mmlu_global_facts': {'acc': True}, 'mmlu_high_school_biology': {'acc': True}, 'mmlu_high_school_chemistry': {'acc': True}, 'mmlu_high_school_computer_science': {'acc': True}, 'mmlu_high_school_european_history': {'acc': True}, 'mmlu_high_school_geography': {'acc': True}, 'mmlu_high_school_government_and_politics': {'acc': True}, 'mmlu_high_school_macroeconomics': {'acc': True}, 'mmlu_high_school_mathematics': {'acc': True}, 'mmlu_high_school_microeconomics': {'acc': True}, 'mmlu_high_school_physics': {'acc': True}, 'mmlu_high_school_psychology': {'acc': True}, 'mmlu_high_school_statistics': {'acc': True}, 'mmlu_high_school_us_history': {'acc': True}, 'mmlu_high_school_world_history': {'acc': True}, 'mmlu_human_aging': {'acc': True}, 'mmlu_human_sexuality': {'acc': True}, 'mmlu_humanities': {'acc': True}, 'mmlu_international_law': {'acc': True}, 'mmlu_jurisprudence': {'acc': True}, 'mmlu_logical_fallacies': {'acc': True}, 'mmlu_machine_learning': {'acc': True}, 'mmlu_management': {'acc': True}, 'mmlu_marketing': {'acc': True}, 'mmlu_medical_genetics': {'acc': True}, 'mmlu_miscellaneous': {'acc': True}, 'mmlu_moral_disputes': {'acc': True}, 'mmlu_moral_scenarios': {'acc': True}, 'mmlu_nutrition': {'acc': True}, 'mmlu_other': {'acc': True}, 'mmlu_philosophy': {'acc': True}, 'mmlu_prehistory': {'acc': True}, 'mmlu_professional_accounting': {'acc': True}, 'mmlu_professional_law': {'acc': True}, 'mmlu_professional_medicine': {'acc': True}, 'mmlu_professional_psychology': {'acc': True}, 'mmlu_public_relations': {'acc': True}, 'mmlu_security_studies': {'acc': True}, 'mmlu_social_sciences': {'acc': True}, 'mmlu_sociology': {'acc': True}, 'mmlu_stem': {'acc': True}, 'mmlu_us_foreign_policy': {'acc': True}, 'mmlu_virology': {'acc': True}, 'mmlu_world_religions': {'acc': True}}, 'n-samples': {'mmlu_logical_fallacies': {'original': 163, 'effective': 163}, 'mmlu_prehistory': {'original': 324, 'effective': 324}, 'mmlu_international_law': {'original': 121, 'effective': 121}, 'mmlu_high_school_us_history': {'original': 204, 'effective': 204}, 'mmlu_formal_logic': {'original': 126, 'effective': 126}, 'mmlu_moral_disputes': {'original': 346, 'effective': 346}, 'mmlu_philosophy': {'original': 311, 'effective': 311}, 'mmlu_high_school_world_history': {'original': 237, 'effective': 237}, 'mmlu_world_religions': {'original': 171, 'effective': 171}, 'mmlu_professional_law': {'original': 1534, 'effective': 1534}, 'mmlu_jurisprudence': {'original': 108, 'effective': 108}, 'mmlu_high_school_european_history': {'original': 165, 'effective': 165}, 'mmlu_moral_scenarios': {'original': 895, 'effective': 895}, 'mmlu_high_school_government_and_politics': {'original': 193, 'effective': 193}, 'mmlu_high_school_macroeconomics': {'original': 390, 'effective': 390}, 'mmlu_professional_psychology': {'original': 612, 'effective': 612}, 'mmlu_econometrics': {'original': 114, 'effective': 114}, 'mmlu_us_foreign_policy': {'original': 100, 'effective': 100}, 'mmlu_public_relations': {'original': 110, 'effective': 110}, 'mmlu_sociology': {'original': 201, 'effective': 201}, 'mmlu_high_school_geography': {'original': 198, 'effective': 198}, 'mmlu_human_sexuality': {'original': 131, 'effective': 131}, 'mmlu_high_school_psychology': {'original': 545, 'effective': 545}, 'mmlu_high_school_microeconomics': {'original': 238, 'effective': 238}, 'mmlu_security_studies': {'original': 245, 'effective': 245}, 'mmlu_professional_accounting': {'original': 282, 'effective': 282}, 'mmlu_virology': {'original': 166, 'effective': 166}, 'mmlu_miscellaneous': {'original': 783, 'effective': 783}, 'mmlu_nutrition': {'original': 306, 'effective': 306}, 'mmlu_global_facts': {'original': 100, 'effective': 100}, 'mmlu_human_aging': {'original': 223, 'effective': 223}, 'mmlu_professional_medicine': {'original': 272, 'effective': 272}, 'mmlu_business_ethics': {'original': 100, 'effective': 100}, 'mmlu_clinical_knowledge': {'original': 265, 'effective': 265}, 'mmlu_marketing': {'original': 234, 'effective': 234}, 'mmlu_medical_genetics': {'original': 100, 'effective': 100}, 'mmlu_college_medicine': {'original': 173, 'effective': 173}, 'mmlu_management': {'original': 103, 'effective': 103}, 'mmlu_abstract_algebra': {'original': 100, 'effective': 100}, 'mmlu_machine_learning': {'original': 112, 'effective': 112}, 'mmlu_college_chemistry': {'original': 100, 'effective': 100}, 'mmlu_high_school_biology': {'original': 310, 'effective': 310}, 'mmlu_high_school_computer_science': {'original': 100, 'effective': 100}, 'mmlu_elementary_mathematics': {'original': 378, 'effective': 378}, 'mmlu_high_school_chemistry': {'original': 203, 'effective': 203}, 'mmlu_electrical_engineering': {'original': 145, 'effective': 145}, 'mmlu_college_biology': {'original': 144, 'effective': 144}, 'mmlu_astronomy': {'original': 152, 'effective': 152}, 'mmlu_college_mathematics': {'original': 100, 'effective': 100}, 'mmlu_college_computer_science': {'original': 100, 'effective': 100}, 'mmlu_conceptual_physics': {'original': 235, 'effective': 235}, 'mmlu_high_school_statistics': {'original': 216, 'effective': 216}, 'mmlu_computer_security': {'original': 100, 'effective': 100}, 'mmlu_anatomy': {'original': 135, 'effective': 135}, 'mmlu_high_school_physics': {'original': 151, 'effective': 151}, 'mmlu_high_school_mathematics': {'original': 270, 'effective': 270}, 'mmlu_college_physics': {'original': 102, 'effective': 102}}, 'config': {'model': 'meta-llama/Llama-2-7b-chat-hf', 'model_args': None, 'model_num_parameters': 6738415616, 'model_dtype': torch.bfloat16, 'model_revision': 'main', 'model_sha': '', 'batch_size': 1, 'batch_sizes': [], 'device': None, 'use_cache': None, 'limit': None, 'bootstrap_iters': 100000, 'gen_kwargs': None, 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}, 'git_hash': '9c9512b', 'date': 1720697594.9916332, 'pretty_env_info': 'PyTorch version: 2.3.0+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Rocky Linux release 8.9 (Green Obsidian) (x86_64)\nGCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.0-20)\nClang version: 16.0.6 (Red Hat 16.0.6-2.module+el8.9.0+1651+e10a8f6d)\nCMake version: version 3.26.5\nLibc version: glibc-2.28\n\nPython version: 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0] (64-bit runtime)\nPython platform: Linux-4.18.0-477.51.1.el8_8.x86_64-x86_64-with-glibc2.28\nIs CUDA available: True\nCUDA runtime version: 11.4.48\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB\nNvidia driver version: 535.161.08\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nByte Order:          Little Endian\nCPU(s):              128\nOn-line CPU(s) list: 0-127\nThread(s) per core:  1\nCore(s) per socket:  64\nSocket(s):           2\nNUMA node(s):        8\nVendor ID:           AuthenticAMD\nCPU family:          25\nModel:               1\nModel name:          AMD EPYC 7763 64-Core Processor\nStepping:            1\nCPU MHz:             2764.150\nBogoMIPS:            4890.88\nVirtualization:      AMD-V\nL1d cache:           32K\nL1i cache:           32K\nL2 cache:            512K\nL3 cache:            32768K\nNUMA node0 CPU(s):   0-15\nNUMA node1 CPU(s):   16-31\nNUMA node2 CPU(s):   32-47\nNUMA node3 CPU(s):   48-63\nNUMA node4 CPU(s):   64-79\nNUMA node5 CPU(s):   80-95\nNUMA node6 CPU(s):   96-111\nNUMA node7 CPU(s):   112-127\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca\n\nVersions of relevant libraries:\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.0\n[pip3] torch-kmeans==0.2.0\n[pip3] triton==2.3.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.0                    pypi_0    pypi\n[conda] torch-kmeans              0.2.0                    pypi_0    pypi\n[conda] triton                    2.3.0                    pypi_0    pypi', 'transformers_version': '4.41.2', 'upper_git_hash': None}
>> task: mmlu / metric name: acc,none / metric val: 0.4715852442671984
